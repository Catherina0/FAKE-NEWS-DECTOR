#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»“æœæ ¼å¼åŒ–æ¨¡å—
è´Ÿè´£å¯¹åˆ†æç»“æœè¿›è¡Œæ ¼å¼åŒ–å’Œæ˜¾ç¤º
"""

import sys
import logging
import traceback
from typing import Dict, Any, Tuple
from config import (
    colorama_available, Fore, Style,
    TITLE_COLOR, HEADER_COLOR, SUBHEADER_COLOR, 
    DETAIL_COLOR, WARNING_COLOR, ERROR_COLOR,
    SUCCESS_COLOR, RESET_COLOR, DEFAULT_WEIGHTS,
    DEEPSEEK_API_AVAILABLE, SEARXNG_AVAILABLE,
    SECTION_COLOR, INFO_COLOR, NEUTRAL_COLOR
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def format_score(score: float) -> str:
    """æ ¼å¼åŒ–è¯„åˆ†ä¸ºä¸¤ä½å°æ•°çš„å­—ç¬¦ä¸²"""
    return f"{float(score):.2f}"

def get_credibility_summary(score: float) -> str:
    """
    æ ¹æ®å¯ä¿¡åº¦è¯„åˆ†ç”Ÿæˆç®€çŸ­æ‘˜è¦
    
    å‚æ•°:
        score (float): å¯ä¿¡åº¦è¯„åˆ† (0-1)
    
    è¿”å›:
        str: å¯ä¿¡åº¦æ‘˜è¦
    """
    try:
        score = float(score)
    except (TypeError, ValueError):
        return "æ— æ³•è¯„ä¼°ï¼šè¯„åˆ†æ•°æ®å¼‚å¸¸"
    
    if score >= 0.8:
        return "æ–°é—»å†…å®¹å¯é æ€§é«˜ï¼Œè®ºæ®å……åˆ†ï¼Œä¿¡æ¯å‡†ç¡®ï¼Œæ¥æºå¯é "
    elif score >= 0.6:
        return "æ–°é—»åŸºæœ¬å¯ä¿¡ï¼Œä½†å»ºè®®æ ¸å®å…³é”®ä¿¡æ¯"
    elif score >= 0.4:
        return "æ–°é—»å¯ä¿¡åº¦å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®è°¨æ…å¯¹å¾…"
    else:
        return "æ–°é—»å¯ä¿¡åº¦ä¸¥é‡ä¸è¶³ï¼Œå¯èƒ½åŒ…å«è™šå‡æˆ–è¯¯å¯¼ä¿¡æ¯"

def get_ai_content_description(score: float) -> str:
    """
    æ ¹æ®AIç”Ÿæˆå†…å®¹è¯„åˆ†æä¾›æè¿°
    
    å‚æ•°:
        score (float): AIç”Ÿæˆè¯„åˆ† (0-1ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šåƒäººç±»å†™ä½œ)
    
    è¿”å›:
        str: æè¿°æ€§æ–‡æœ¬
    """
    try:
        score = float(score)
    except (TypeError, ValueError):
        return "æ— æ³•è¯„ä¼°AIç”Ÿæˆå¯èƒ½æ€§"
    
    if score >= 0.85:
        return "æ–‡æœ¬é«˜åº¦ç¬¦åˆäººç±»å†™ä½œç‰¹å¾ï¼ŒAIç”Ÿæˆå¯èƒ½æ€§å¾ˆä½"
    elif score >= 0.7:
        return "æ–‡æœ¬æ•´ä½“ç¬¦åˆäººç±»å†™ä½œç‰¹å¾ï¼ŒAIç”Ÿæˆå¯èƒ½æ€§è¾ƒä½"
    elif score >= 0.5:
        return "æ–‡æœ¬æœ‰éƒ¨åˆ†AIç”Ÿæˆç‰¹å¾ï¼Œä½†ä»ä¿ç•™äººç±»å†™ä½œé£æ ¼"
    elif score >= 0.3:
        return "æ–‡æœ¬å‘ˆç°æ˜æ˜¾çš„AIç”Ÿæˆç‰¹å¾ï¼Œå¯èƒ½æ˜¯AIè¾…åŠ©åˆ›ä½œ"
    else:
        return "æ–‡æœ¬ææœ‰å¯èƒ½ç”±AIç”Ÿæˆï¼Œäººç±»å†™ä½œç‰¹å¾æå°‘"

def get_rating_emoji(score):
    """æ ¹æ®è¯„åˆ†è¿”å›å¯¹åº”çš„emojiå’Œè¯„çº§"""
    if score >= 0.8:
        return "ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ", "ä¼˜"
    elif score >= 0.7:
        return "ğŸŒŸğŸŒŸğŸŒŸ", "è‰¯"
    elif score >= 0.6:
        return "ğŸŒŸğŸŒŸ", "ä¸­"
    elif score >= 0.5:
        return "ğŸŒŸ", "ä¸€èˆ¬"
    else:
        return "â—", "å·®"

def get_progress_bar(score, width=10):
    """ç”Ÿæˆè¿›åº¦æ¡"""
    if score is None:
        # å¯¹äºNoneå€¼ï¼Œè¿”å›ç©ºè¿›åº¦æ¡
        return f"{'â–‘' * width}"
    
    # ç¡®ä¿scoreæ˜¯æµ®ç‚¹æ•°
    try:
        score_float = float(score)
    except (TypeError, ValueError):
        # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œè¿”å›ç©ºè¿›åº¦æ¡
        return f"{'â–‘' * width}"
    
    # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
    score_float = max(0.0, min(1.0, score_float))
    
    filled = int(score_float * width)
    return f"{'â–ˆ' * filled}{'â–‘' * (width - filled)}"

def get_credibility_rating(score):
    """æ ¹æ®å¯ä¿¡åº¦è¯„åˆ†è¿”å›è¯„çº§"""
    if score >= 0.8:
        return f"{TITLE_COLOR}é«˜åº¦å¯ä¿¡ ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ{RESET_COLOR}", "é«˜"
    elif score >= 0.6:
        return f"{SUCCESS_COLOR}éƒ¨åˆ†å¯ä¿¡ ğŸŒŸğŸŒŸ{RESET_COLOR}", "ä¸­"
    elif score >= 0.4:
        return f"{WARNING_COLOR}ä½åº¦å¯ä¿¡ ğŸŒŸ{RESET_COLOR}", "ä½" 
    else:
        return f"{ERROR_COLOR}ä¸å¯ä¿¡ â—{RESET_COLOR}", "æä½"

def validate_score(score: Any, source: str = "æœªçŸ¥") -> float:
    """éªŒè¯å¹¶è½¬æ¢è¯„åˆ†"""
    try:
        score_float = float(score)
        if not 0 <= score_float <= 1:
            logger.warning(f"è¯„åˆ†è¶…å‡ºèŒƒå›´[0-1]: {score_float} (æ¥æº: {source})")
            return max(0.0, min(1.0, score_float))
        return score_float
    except (TypeError, ValueError) as e:
        logger.error(f"è¯„åˆ†è½¬æ¢å¤±è´¥: {score} (æ¥æº: {source}) - {str(e)}")
        raise ValueError(f"æ— æ•ˆçš„è¯„åˆ†å€¼: {score}")

def validate_data(data: Dict[str, Any], required_fields: list, context: str = "") -> bool:
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    missing_fields = [field for field in required_fields if field not in data]
    if missing_fields:
        logger.warning(f"{context} - ç¼ºå¤±å­—æ®µ: {', '.join(missing_fields)}")
        return False
    return True

def calculate_weighted_score(main_scores: dict, cross_validation_data: dict = None) -> Tuple[float, dict]:
    """
    è®¡ç®—åŠ æƒæ€»åˆ†
    
    å‚æ•°:
        main_scores: åŒ…å«å„ç»´åº¦è¯„åˆ†çš„å­—å…¸
        cross_validation_data: äº¤å‰éªŒè¯æ•°æ®
    
    è¿”å›:
        Tuple[float, dict]: (åŠ æƒæ€»åˆ†, å„ç»´åº¦æƒé‡)
    """
    # å®šä¹‰å„ç»´åº¦çš„æƒé‡ï¼ˆæ€»å’Œä¸º0.7ï¼Œä¸ºäº¤å‰éªŒè¯é¢„ç•™0.3ï¼‰
    weights = {
        "å†…å®¹çœŸå®æ€§": 0.15,  # 
        "ä¿¡æ¯å‡†ç¡®æ€§": 0.15,  #
        "æ¥æºå¯é æ€§": 0.15,  #
        "å¼•ç”¨è´¨é‡": 0.10,    #
        "è¯­è¨€å®¢è§‚æ€§": 0.08,  #
        "é€»è¾‘è¿è´¯æ€§": 0.07,  
        "äº¤å‰éªŒè¯": 0.30     # æ–°å¢äº¤å‰éªŒè¯æƒé‡
    }
    
    total_weight = 0
    weighted_sum = 0
    used_weights = {}
    
    # è®°å½•è¾“å…¥æ•°æ®ï¼Œå¸®åŠ©è°ƒè¯•
    logger.debug(f"è®¡ç®—åŠ æƒåˆ†æ•° - ä¸»è¦è¯„åˆ†: {main_scores}")
    logger.debug(f"è®¡ç®—åŠ æƒåˆ†æ•° - äº¤å‰éªŒè¯æ•°æ®: {cross_validation_data}")
    
    # å¤„ç†ä¸»è¦ç»´åº¦è¯„åˆ†
    for dimension, weight in weights.items():
        if dimension == "äº¤å‰éªŒè¯":
            continue  # è·³è¿‡äº¤å‰éªŒè¯ï¼Œç¨åå¤„ç†
        if dimension in main_scores:
            try:
                score = validate_score(main_scores[dimension], f"ç»´åº¦è¯„åˆ†.{dimension}")
                weighted_sum += score * weight
                total_weight += weight
                used_weights[dimension] = weight
                logger.debug(f"{dimension} è¯„åˆ†: {score:.2f}, æƒé‡: {weight}")
            except ValueError:
                logger.warning(f"è·³è¿‡æ— æ•ˆçš„è¯„åˆ†é¡¹: {dimension}")
                continue
    
    # å¤„ç†äº¤å‰éªŒè¯è¯„åˆ†
    if cross_validation_data and isinstance(cross_validation_data, dict):
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥ä½¿ç”¨scoreå­—æ®µ
            if "score" in cross_validation_data:
                cross_validation_score = validate_score(cross_validation_data["score"], "äº¤å‰éªŒè¯.score")
                logger.debug(f"ä½¿ç”¨äº¤å‰éªŒè¯ç›´æ¥æä¾›çš„è¯„åˆ†: {cross_validation_score}")
            else:
                # å¦‚æœæ²¡æœ‰scoreå­—æ®µï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæ¨æ–­
                cross_validation_score = 0.0
                
                # ä»ä¸€è‡´æ€§æ•°æ®ä¸­æå–
                consistency = cross_validation_data.get("consistency", cross_validation_data.get("ä¸€è‡´æ€§", 0))
                if consistency:
                    try:
                        if isinstance(consistency, str) and "%" in consistency:
                            # å¤„ç†ç™¾åˆ†æ¯”å­—ç¬¦ä¸²
                            consistency = float(consistency.strip("%")) / 100
                        else:
                            consistency = float(consistency)
                        cross_validation_score = consistency
                        logger.debug(f"ä»ä¸€è‡´æ€§æ•°æ®ä¸­æå–äº¤å‰éªŒè¯è¯„åˆ†: {cross_validation_score}")
                    except (ValueError, TypeError):
                        logger.warning(f"æ— æ³•ä»ä¸€è‡´æ€§æ•°æ®ä¸­æå–è¯„åˆ†: {consistency}")
                
                # ä»æ¥æºå¯ä¿¡åº¦ä¸­æå–
                if cross_validation_score == 0.0:
                    source_credibility = cross_validation_data.get("source_credibility", cross_validation_data.get("æ¥æºå¯ä¿¡åº¦", ""))
                    if isinstance(source_credibility, str):
                        if "é«˜åº¦å¯ä¿¡" in source_credibility:
                            cross_validation_score = 0.9
                        elif "å¯ä¿¡" in source_credibility:
                            cross_validation_score = 0.7
                        elif "éƒ¨åˆ†å¯ä¿¡" in source_credibility:
                            cross_validation_score = 0.5
                        elif "ä½å¯ä¿¡" in source_credibility:
                            cross_validation_score = 0.3
                        else:
                            cross_validation_score = 0.1
                        logger.debug(f"ä»source_credibilityæå–äº¤å‰éªŒè¯è¯„åˆ†: {cross_validation_score}")
            
            # è€ƒè™‘æ¥æºæ•°é‡å½±å“è¯„åˆ†
            source_count = cross_validation_data.get("source_count", 0)
            if source_count > 5:
                cross_validation_score = min(1.0, cross_validation_score * 1.2)
                logger.debug(f"æ¥æºæ•°é‡è¾ƒå¤šï¼Œæå‡è¯„åˆ†è‡³: {cross_validation_score}")
            elif source_count < 2:
                cross_validation_score = cross_validation_score * 0.8
                logger.debug(f"æ¥æºæ•°é‡è¾ƒå°‘ï¼Œé™ä½è¯„åˆ†è‡³: {cross_validation_score}")
            logger.debug(f"è€ƒè™‘æ¥æºæ•°é‡åçš„äº¤å‰éªŒè¯è¯„åˆ†: {cross_validation_score}")
            
            # åˆ›å»ºä¸€ä¸ªè‡³å°‘0.4çš„é»˜è®¤å¾—åˆ†ï¼Œé¿å…æ— æ•ˆè¯„åˆ†
            if cross_validation_score <= 0:
                cross_validation_score = 0.4
                logger.warning(f"äº¤å‰éªŒè¯è¯„åˆ†æ— æ•ˆæˆ–è¿‡ä½ï¼Œä½¿ç”¨é»˜è®¤å€¼: {cross_validation_score}")
            
            weighted_sum += cross_validation_score * weights["äº¤å‰éªŒè¯"]
            total_weight += weights["äº¤å‰éªŒè¯"]
            used_weights["äº¤å‰éªŒè¯"] = weights["äº¤å‰éªŒè¯"]
            logger.debug(f"äº¤å‰éªŒè¯è¯„åˆ†: {cross_validation_score:.2f}, æƒé‡: {weights['äº¤å‰éªŒè¯']}")
        except Exception as e:
            logger.warning(f"å¤„ç†äº¤å‰éªŒè¯è¯„åˆ†æ—¶å‡ºé”™: {str(e)}")
            # ä½¿ç”¨é»˜è®¤è¯„åˆ†
            cross_validation_score = 0.5
            logger.info(f"äº¤å‰éªŒè¯å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†: {cross_validation_score}")
            weighted_sum += cross_validation_score * weights["äº¤å‰éªŒè¯"]
            total_weight += weights["äº¤å‰éªŒè¯"]
            used_weights["äº¤å‰éªŒè¯"] = weights["äº¤å‰éªŒè¯"]
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¯„åˆ†é¡¹ï¼Œä½†æœ‰äº¤å‰éªŒè¯æ•°æ®
    if total_weight == 0 and cross_validation_data:
        logger.warning("ä¸»è¦è¯„åˆ†é¡¹ä¸ºç©ºï¼Œå°è¯•å•ç‹¬ä½¿ç”¨äº¤å‰éªŒè¯æ•°æ®")
        try:
            # å°è¯•ä»äº¤å‰éªŒè¯ä¸­æå–è¯„åˆ†
            if hasattr(cross_validation_data, 'get'):
                cv_score = cross_validation_data.get('score', 0.7)  # é»˜è®¤ä½¿ç”¨0.7
                return float(cv_score), {"äº¤å‰éªŒè¯": 1.0}
        except Exception as e:
            logger.error(f"å°è¯•ä½¿ç”¨äº¤å‰éªŒè¯æ•°æ®å¤±è´¥: {str(e)}")
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¯„åˆ†é¡¹ï¼Œè¿”å›None
    if total_weight == 0:
        logger.warning("æ²¡æœ‰æœ‰æ•ˆè¯„åˆ†é¡¹ï¼Œè¿”å›None")
        return None, {}
    
    # é‡æ–°å½’ä¸€åŒ–æƒé‡
    if total_weight < 1.0:
        normalization_factor = 1.0 / total_weight
        weighted_sum *= normalization_factor
        used_weights = {k: v * normalization_factor for k, v in used_weights.items()}
    
    logger.info(f"è®¡ç®—å¾—åˆ°åŠ æƒæ€»åˆ†: {weighted_sum:.2f}")
    return weighted_sum, used_weights

def analyze_problems(result: dict, total_score: float, main_scores: dict, cross_validation_data: dict) -> list:
    """
    AIåˆ†æå­˜åœ¨çš„é—®é¢˜
    
    å‚æ•°:
        result: å®Œæ•´çš„åˆ†æç»“æœ
        total_score: æ€»åˆ†
        main_scores: ä¸»è¦ç»´åº¦è¯„åˆ†
        cross_validation_data: äº¤å‰éªŒè¯æ•°æ®
    
    è¿”å›:
        list: é—®é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªé—®é¢˜æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä¸¥é‡ç¨‹åº¦ã€æè¿°å’Œå»ºè®®
    """
    problems = []
    
    # 1. åˆ†ææ€»ä½“å¯ä¿¡åº¦
    if total_score < 0.4:
        problems.append({
            "severity": "ä¸¥é‡",
            "type": "æ€»ä½“å¯ä¿¡åº¦",
            "description": f"æ–°é—»æ•´ä½“å¯ä¿¡åº¦æä½ ({total_score:.1%})",
            "suggestion": "å»ºè®®è°¨æ…å¯¹å¾…è¯¥æ–°é—»å†…å®¹ï¼Œéœ€è¦å¤§é‡é¢å¤–éªŒè¯",
            "color": ERROR_COLOR
        })
    elif total_score < 0.6:
        problems.append({
            "severity": "ä¸­ç­‰",
            "type": "æ€»ä½“å¯ä¿¡åº¦",
            "description": f"æ–°é—»å¯ä¿¡åº¦è¾ƒä½ ({total_score:.1%})",
            "suggestion": "å»ºè®®è¿›ä¸€æ­¥æ ¸å®å…³é”®ä¿¡æ¯",
            "color": WARNING_COLOR
        })
    
    # 2. åˆ†æå„ç»´åº¦è¯„åˆ†
    dimension_thresholds = {
        "å†…å®¹çœŸå®æ€§": (0.6, "æ ¸å®æ–°é—»ä¸­çš„å…³é”®äº‹å®å’Œæ•°æ®"),
        "ä¿¡æ¯å‡†ç¡®æ€§": (0.6, "æ£€æŸ¥ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§"),
        "æ¥æºå¯é æ€§": (0.6, "éªŒè¯ä¿¡æ¯æ¥æºçš„æƒå¨æ€§"),
        "å¼•ç”¨è´¨é‡": (0.6, "æ£€æŸ¥å¼•ç”¨çš„å‡†ç¡®æ€§å’Œå¯é æ€§"),
        "è¯­è¨€å®¢è§‚æ€§": (0.5, "æ³¨æ„å¯èƒ½å­˜åœ¨çš„ä¸»è§‚åè§"),
        "é€»è¾‘è¿è´¯æ€§": (0.5, "æ£€æŸ¥å†…å®¹çš„é€»è¾‘æ€§å’Œè¿è´¯æ€§")
    }
    
    for dim, (threshold, suggestion) in dimension_thresholds.items():
        if dim in main_scores:
            score = float(main_scores[dim])
            if score < threshold:
                severity = "ä¸¥é‡" if score < 0.4 else "ä¸­ç­‰"
                problems.append({
                    "severity": severity,
                    "type": dim,
                    "description": f"{dim}è¯„åˆ†è¿‡ä½ ({score:.1%})",
                    "suggestion": suggestion,
                    "color": ERROR_COLOR if score < 0.4 else WARNING_COLOR
                })
    
    # 3. åˆ†æäº¤å‰éªŒè¯
    if cross_validation_data and isinstance(cross_validation_data, dict):
        # æ”¹è¿›æºè®¡æ•°æ£€æµ‹ï¼Œä¼˜å…ˆä½¿ç”¨æ¥æºæ•°é‡æˆ–æœç´¢ç»“æœæ€»æ•°
        source_count = 0
        search_results_count = 0
        
        # å°è¯•å¤šç§å¯èƒ½çš„é”®åè·å–æ¥æºæ•°é‡
        for key in ["source_count", "sources_count", "æœç´¢ç»“æœæ€»æ•°", "æ¥æºæ•°é‡", "ç›¸å…³æ¥æºæ•°"]:
            if key in cross_validation_data and isinstance(cross_validation_data[key], (int, float, str)):
                try:
                    source_count = int(cross_validation_data[key])
                    logger.info(f"ä»é”®å '{key}' è·å–åˆ°æ¥æºæ•°é‡: {source_count}")
                    break
                except (ValueError, TypeError):
                    logger.warning(f"æ— æ³•å°† {key}:{cross_validation_data[key]} è½¬æ¢ä¸ºæ•´æ•°")
        
        # å¦‚æœæ‰¾ä¸åˆ°ç›´æ¥çš„è®¡æ•°å­—æ®µï¼Œå°è¯•ä»sourcesåˆ—è¡¨è·å–
        if source_count == 0 and "sources" in cross_validation_data and isinstance(cross_validation_data["sources"], list):
            source_count = len(cross_validation_data["sources"])
            logger.info(f"ä»sourcesåˆ—è¡¨é•¿åº¦è·å–æ¥æºæ•°é‡: {source_count}")
        
        # å¦‚æœæ‰¾ä¸åˆ°sourcesåˆ—è¡¨ï¼Œå°è¯•ä»ç›¸å…³æ¥æºè·å–
        if source_count == 0 and "ç›¸å…³æ¥æº" in cross_validation_data and isinstance(cross_validation_data["ç›¸å…³æ¥æº"], list):
            source_count = len(cross_validation_data["ç›¸å…³æ¥æº"])
            logger.info(f"ä»ç›¸å…³æ¥æºåˆ—è¡¨é•¿åº¦è·å–æ¥æºæ•°é‡: {source_count}")
        
        # å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³æ¥æºï¼Œå°è¯•ä»verified_sourcesè·å–
        if source_count == 0 and "verified_sources" in cross_validation_data and isinstance(cross_validation_data["verified_sources"], list):
            source_count = len(cross_validation_data["verified_sources"])
            logger.info(f"ä»verified_sourcesåˆ—è¡¨é•¿åº¦è·å–æ¥æºæ•°é‡: {source_count}")
        
        # æ£€æŸ¥éªŒè¯ç‚¹æ˜¯å¦å­˜åœ¨
        no_result_count = 0
        verification_points = []
        
        # å°è¯•è·å–éªŒè¯ç‚¹åˆ—è¡¨å’Œæœç´¢ç»“æœè®¡æ•°
        if "éªŒè¯ç‚¹" in cross_validation_data and isinstance(cross_validation_data["éªŒè¯ç‚¹"], list):
            verification_points = cross_validation_data["éªŒè¯ç‚¹"]
            # è®¡ç®—æ²¡æœ‰æœç´¢ç»“æœçš„éªŒè¯ç‚¹æ•°é‡
            no_result_count = sum(1 for p in verification_points if isinstance(p, dict) and p.get("æœç´¢ç»“æœæ•°é‡", 0) == 0)
            logger.info(f"ä»éªŒè¯ç‚¹ä¸­å‘ç°æ— æœç´¢ç»“æœçš„ç‚¹æ•°: {no_result_count}")
            
            # å°è¯•ä»éªŒè¯ç‚¹ä¸­è·å–æœç´¢ç»“æœæ€»æ•°
            for p in verification_points:
                if isinstance(p, dict):
                    if "æœç´¢ç»“æœ" in p and isinstance(p["æœç´¢ç»“æœ"], int):
                        search_results_count += p["æœç´¢ç»“æœ"]
                    elif "æœç´¢ç»“æœæ•°é‡" in p and isinstance(p["æœç´¢ç»“æœæ•°é‡"], int):
                        search_results_count += p["æœç´¢ç»“æœæ•°é‡"]
                        
        elif "verification_points" in cross_validation_data and isinstance(cross_validation_data["verification_points"], list):
            verification_points = cross_validation_data["verification_points"]
            # è®¡ç®—æ²¡æœ‰æœç´¢ç»“æœçš„éªŒè¯ç‚¹æ•°é‡
            no_result_count = sum(1 for p in verification_points if isinstance(p, dict) and p.get("æœç´¢ç»“æœæ•°é‡", 0) == 0)
            logger.info(f"ä»verification_pointsä¸­å‘ç°æ— æœç´¢ç»“æœçš„ç‚¹æ•°: {no_result_count}")
            
            # å°è¯•ä»éªŒè¯ç‚¹ä¸­è·å–æœç´¢ç»“æœæ€»æ•°
            for p in verification_points:
                if isinstance(p, dict):
                    if "æœç´¢ç»“æœ" in p and isinstance(p["æœç´¢ç»“æœ"], int):
                        search_results_count += p["æœç´¢ç»“æœ"]
                    elif "æœç´¢ç»“æœæ•°é‡" in p and isinstance(p["æœç´¢ç»“æœæ•°é‡"], int):
                        search_results_count += p["æœç´¢ç»“æœæ•°é‡"]
                        
        elif "claims" in cross_validation_data and isinstance(cross_validation_data["claims"], list):
            verification_points = cross_validation_data["claims"]
            # è®¡ç®—æ²¡æœ‰æœç´¢ç»“æœçš„éªŒè¯ç‚¹æ•°é‡
            no_result_count = sum(1 for p in verification_points if isinstance(p, dict) and p.get("æœç´¢ç»“æœæ•°é‡", 0) == 0)
            logger.info(f"ä»claimsä¸­å‘ç°æ— æœç´¢ç»“æœçš„ç‚¹æ•°: {no_result_count}")
            
            # å°è¯•ä»éªŒè¯ç‚¹ä¸­è·å–æœç´¢ç»“æœæ€»æ•°
            for p in verification_points:
                if isinstance(p, dict):
                    if "æœç´¢ç»“æœ" in p and isinstance(p["æœç´¢ç»“æœ"], int):
                        search_results_count += p["æœç´¢ç»“æœ"]
                    elif "æœç´¢ç»“æœæ•°é‡" in p and isinstance(p["æœç´¢ç»“æœæ•°é‡"], int):
                        search_results_count += p["æœç´¢ç»“æœæ•°é‡"]
        
        # å¦‚æœæœç´¢ç»“æœæ•°å¤§äº0ä½†æ¥æºè®¡æ•°ä¸º0ï¼Œä½¿ç”¨æœç´¢ç»“æœæ•°ä½œä¸ºæ¥æºè®¡æ•°çš„ä¼°è®¡
        if search_results_count > 0 and source_count == 0:
            logger.info(f"ä½¿ç”¨æœç´¢ç»“æœæ•°é‡({search_results_count})ä½œä¸ºæ¥æºæ•°é‡ä¼°è®¡")
            source_count = search_results_count
            
        # æ·»åŠ æ¥æºä¸è¶³é—®é¢˜ï¼ˆä»…å½“æœç´¢ç»“æœç¡®å®ä¸è¶³æ—¶ï¼‰
        if source_count < 2 and search_results_count < 3:
            problems.append({
                "severity": "ä¸­ç­‰",
                "type": "äº¤å‰éªŒè¯",
                "description": f"ç¼ºä¹è¶³å¤Ÿçš„äº¤å‰éªŒè¯æ¥æº (ä»…{source_count}ä¸ª)",
                "suggestion": "å»ºè®®å¯»æ‰¾æ›´å¤šç‹¬ç«‹æ¥æºéªŒè¯ä¿¡æ¯",
                "color": WARNING_COLOR
            })
        
        # æ·»åŠ éªŒè¯ç‚¹é—®é¢˜
        if no_result_count > 0:
            verification_points_count = len(verification_points) if verification_points else 0
            problems.append({
                "severity": "ä¸­ç­‰",
                "type": "äº¤å‰éªŒè¯å®Œæ•´æ€§",
                "description": f"{no_result_count}ä¸ªéªŒè¯ç‚¹æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ (å…±{verification_points_count}ä¸ªéªŒè¯ç‚¹)",
                "suggestion": "å»ºè®®é’ˆå¯¹è¿™äº›ç‰¹å®šä¿¡æ¯ç‚¹è¿›è¡Œé¢å¤–éªŒè¯",
                "color": WARNING_COLOR
            })
        
        # åˆ†ææ¥æºå¯ä¿¡åº¦
        credibility = cross_validation_data.get("source_credibility", "")
        if isinstance(credibility, str) and ("ä½å¯ä¿¡" in credibility or "ä¸å¯ä¿¡" in credibility):
            problems.append({
                "severity": "ä¸¥é‡",
                "type": "äº¤å‰éªŒè¯",
                "description": "äº¤å‰éªŒè¯æ¥æºå¯ä¿¡åº¦ä½",
                "suggestion": "å»ºè®®å¯»æ‰¾æ›´æƒå¨çš„ä¿¡æ¯æ¥æº",
                "color": ERROR_COLOR
            })
    
    return problems

def print_problems_section(problems: list):
    """æ‰“å°é—®é¢˜åˆ†æéƒ¨åˆ†"""
    print(f"\n{SUBHEADER_COLOR}å››ã€é—®é¢˜ç‚¹åˆ†æ{RESET_COLOR}")
    print(f"{DETAIL_COLOR}{'â”' * 70}{RESET_COLOR}")
    
    if not problems:
        print(f"{SUCCESS_COLOR}  âœ“ æœªå‘ç°æ˜æ˜¾é—®é¢˜{RESET_COLOR}")
        print(f"{DETAIL_COLOR}  â€¢ å»ºè®®ï¼šä¿æŒæ‰¹åˆ¤æ€§æ€ç»´ï¼Œå…³æ³¨ä¿¡æ¯æ›´æ–°{RESET_COLOR}")
        return
    
    # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼ˆä¸¥é‡ > ä¸­ç­‰ï¼‰
    problems.sort(key=lambda x: 0 if x["severity"] == "ä¸¥é‡" else 1)
    
    for i, problem in enumerate(problems, 1):
        color = problem["color"]
        print(f"\n{color}{i}. {problem['type']}é—®é¢˜:{RESET_COLOR}")
        print(f"{color}  âš ï¸ ä¸¥é‡æ€§ï¼š{problem['severity']}{RESET_COLOR}")
        print(f"{color}    - {problem['description']}{RESET_COLOR}")
        print(f"{color}    - å»ºè®®ï¼š{problem['suggestion']}{RESET_COLOR}")

def print_formatted_result(result: Dict[str, Any], colored_output: bool = True) -> None:
    """
    æ ¼å¼åŒ–æ‰“å°åˆ†æç»“æœ
    
    å‚æ•°:
        result (Dict[str, Any]): åˆ†æç»“æœå­—å…¸
        colored_output (bool): æ˜¯å¦ä½¿ç”¨å½©è‰²è¾“å‡º
    """
    try:
        logger.info("å¼€å§‹æ ¼å¼åŒ–åˆ†æç»“æœ")
        logger.debug(f"è¾“å…¥æ•°æ®: {result}")
        
        # éªŒè¯è¾“å…¥æ•°æ®
        if not isinstance(result, dict):
            raise TypeError(f"è¾“å…¥æ•°æ®ç±»å‹é”™è¯¯: æœŸæœ› dict, è·å¾— {type(result)}")
        
        # 1. å¤„ç†æ€»ä½“è¯„åˆ† - ä½¿ç”¨åŠ æƒè®¡ç®—
        logger.debug("å¼€å§‹å¤„ç†æ€»ä½“è¯„åˆ†")
        
        # è·å–ä¸»è¦è¯„åˆ†æ•°æ®
        main_scores = {}
        scoring_details = result.get("è¯„åˆ†è¯¦æƒ…", {})
        deepseek_data = result.get("åŸå§‹åˆ†ææ•°æ®", {}).get("deepseek_full_response", {})
        
        # æå–AIç”Ÿæˆå†…å®¹æ£€æµ‹æ•°æ®
        ai_content_data = None
        # å°è¯•å¤šä¸ªå¯èƒ½çš„é”®åç§°
        for key in ["AIç”Ÿæˆå†…å®¹", "ai_content", "AI_content", "ai_generation_detection", "AIç”Ÿæˆæ£€æµ‹"]:
            if key in result and result[key]:
                ai_content_data = result[key]
                logger.info(f"æ‰¾åˆ°AIç”Ÿæˆå†…å®¹æ£€æµ‹æ•°æ®ï¼Œé”®å: {key}")
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»deepseekæ•°æ®ä¸­æå–
        if not ai_content_data and isinstance(deepseek_data, dict):
            for key in ["AIç”Ÿæˆå†…å®¹", "ai_content", "AI_content", "ai_generation", "aiç‡"]:
                if key in deepseek_data:
                    ai_content_data = deepseek_data[key]
                    logger.info(f"ä»deepseekæ•°æ®ä¸­æå–AIç”Ÿæˆå†…å®¹æ£€æµ‹æ•°æ®ï¼Œé”®å: {key}")
                    break
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œåˆ™æœç´¢æ‰€æœ‰å¯èƒ½åŒ…å«AIæ£€æµ‹ç›¸å…³ä¿¡æ¯çš„é”®
            if not ai_content_data:
                for key in deepseek_data:
                    if isinstance(deepseek_data[key], dict) and any(ai_term in key.lower() for ai_term in ["ai", "äººå·¥æ™ºèƒ½", "æœºå™¨", "ç”Ÿæˆ"]):
                        ai_content_data = deepseek_data[key]
                        logger.info(f"ä»deepseekæ•°æ®é€šè¿‡å…³é”®è¯æœç´¢æ‰¾åˆ°AIç”Ÿæˆå†…å®¹æ£€æµ‹æ•°æ®ï¼Œé”®å: {key}")
                        break
        
        # è®°å½•æ‰¾åˆ°çš„AIç”Ÿæˆå†…å®¹æ£€æµ‹æ•°æ®
        logger.debug(f"æå–çš„AIç”Ÿæˆå†…å®¹æ£€æµ‹æ•°æ®: {ai_content_data}")
        
        # æ›´å…¨é¢åœ°å°è¯•æå–äº¤å‰éªŒè¯æ•°æ®
        cross_validation_data = None
        # å°è¯•å¤šä¸ªå¯èƒ½çš„é”®åç§°
        for key in ["cross_validation", "äº¤å‰éªŒè¯", "crossValidation", "cross-validation", "validation_results"]:
            if key in result and result[key]:
                cross_validation_data = result[key]
                logger.info(f"æ‰¾åˆ°äº¤å‰éªŒè¯æ•°æ®ï¼Œé”®å: {key}")
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°äº¤å‰éªŒè¯æ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ—¥å¿—ä¿¡æ¯å¯ä»¥æå–
        if not cross_validation_data:
            logger.warning("åœ¨ç»“æœå­—å…¸ä¸­æœªæ‰¾åˆ°äº¤å‰éªŒè¯æ•°æ®ï¼Œå°è¯•ä»å…¶ä»–ä½ç½®æå–")
            # å°è¯•ä»validationå­—æ®µæˆ–å…¶ä»–å¯èƒ½åŒ…å«äº¤å‰éªŒè¯ä¿¡æ¯çš„å­—æ®µä¸­æå–
            for key in ["validation", "verification", "å¤–éƒ¨éªŒè¯", "ä¿¡æ¯éªŒè¯", "web_validation"]:
                if key in result:
                    cross_validation_data = result[key]
                    logger.info(f"ä»å…¶ä»–ä½ç½®æ‰¾åˆ°äº¤å‰éªŒè¯æ•°æ®ï¼Œé”®å: {key}")
                    break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»deepseek_dataä¸­æŸ¥æ‰¾
            if not cross_validation_data and isinstance(deepseek_data, dict):
                for key in ["äº¤å‰éªŒè¯", "éªŒè¯", "cross_validation"]:
                    if key in deepseek_data:
                        cross_validation_data = deepseek_data[key]
                        logger.info(f"ä»deepseekæ•°æ®ä¸­æ‰¾åˆ°äº¤å‰éªŒè¯æ•°æ®ï¼Œé”®å: {key}")
                        break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬ç»“æ„ç”¨äºæ˜¾ç¤º
            if not cross_validation_data and "cv_score" in result:
                cross_validation_data = {
                    "score": result["cv_score"],
                    "source_count": result.get("cv_source_count", 0),
                    "unique_sources": result.get("cv_unique_sources", 0),
                    "source_credibility": result.get("cv_credibility", "æœªçŸ¥"),
                    "timeliness": result.get("cv_timeliness", "æœªçŸ¥")
                }
                logger.info("ä»åˆ†æ•£å­—æ®µæ„å»ºäº¤å‰éªŒè¯æ•°æ®")
        
        # è®°å½•æ‰¾åˆ°çš„äº¤å‰éªŒè¯æ•°æ®
        logger.debug(f"æå–çš„äº¤å‰éªŒè¯æ•°æ®: {cross_validation_data}")
        
        # ä»å¤šä¸ªæ¥æºæ”¶é›†è¯„åˆ†æ•°æ®
        if isinstance(scoring_details, dict):
            for key, value in scoring_details.items():
                if key.startswith(("å†…å®¹çœŸå®æ€§", "ä¿¡æ¯å‡†ç¡®æ€§", "æ¥æºå¯é æ€§", "å¼•ç”¨è´¨é‡", "è¯­è¨€å®¢è§‚æ€§", "é€»è¾‘è¿è´¯æ€§")):
                    clean_key = key.split("_")[-1] if "_" in key else key
                    main_scores[clean_key] = value
        
        # å¦‚æœè¯„åˆ†è¯¦æƒ…ä¸­æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä»deepseekæ•°æ®è·å–
        if not main_scores and isinstance(deepseek_data, dict):
            main_scores = deepseek_data.get("å„å¤§ç±»è¯„åˆ†", {})
        
        # è®¡ç®—åŠ æƒæ€»åˆ†ï¼ŒåŒ…å«äº¤å‰éªŒè¯
        total_score, weights = calculate_weighted_score(main_scores, cross_validation_data)
        
        if total_score is None:
            error_msg = "æ— æ³•è®¡ç®—åŠ æƒæ€»åˆ†ï¼Œè¯„åˆ†æ•°æ®æ— æ•ˆ"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        total_score_pct = total_score * 100
        score_source = "åŠ æƒè®¡ç®—"
        logger.info(f"æœ€ç»ˆæ€»åˆ†: {total_score_pct:.1f}% (æ¥æº: {score_source})")
        
        # è®°å½•ä½¿ç”¨çš„æƒé‡
        logger.info("ä½¿ç”¨çš„ç»´åº¦æƒé‡:")
        for dimension, weight in weights.items():
            logger.info(f"  {dimension}: {weight:.2f}")
        
        # è·å–è¯„çº§
        rating_text, rating_level = get_credibility_rating(total_score)
        logger.debug(f"å¯ä¿¡åº¦è¯„çº§: {rating_text} (çº§åˆ«: {rating_level})")
        
        # é¡¶éƒ¨æ¨ªå¹…
        print(f"\n{HEADER_COLOR}{'=' * 70}{RESET_COLOR}")
        print(f"{HEADER_COLOR}{'ğŸ“Š æ–°é—»å¯ä¿¡åº¦åˆ†ææŠ¥å‘Š ğŸ“Š':^70}{RESET_COLOR}")
        print(f"{HEADER_COLOR}{'=' * 70}{RESET_COLOR}")
        
        # æ€»è¯„éƒ¨åˆ†
        print(f"\n{TITLE_COLOR}{'â–“' * 70}{RESET_COLOR}")
        print(f"{TITLE_COLOR}{'æ€»ä½“å¯ä¿¡åº¦è¯„çº§: ' + rating_text:^70}{RESET_COLOR}")
        print(f"{TITLE_COLOR}{f'æ€»åˆ†: {total_score_pct:.1f}% (æ¥æº: {score_source})':^70}{RESET_COLOR}")
        print(f"{TITLE_COLOR}{'â–“' * 70}{RESET_COLOR}")
        
        # è·å–æ‘˜è¦
        summary = get_credibility_summary(total_score)
        logger.debug(f"ç”Ÿæˆæ€»ä½“è¯„ä¼°æ‘˜è¦: {summary}")
        print(f"\n{SECTION_COLOR}ã€– æ€»ä½“è¯„ä¼° ã€—{RESET_COLOR}")
        print(f"{DETAIL_COLOR}{summary}{RESET_COLOR}")
        
        # ä¸€ã€å†…å®¹çœŸå®æ€§ä¸å‡†ç¡®æ€§åˆ†æ
        print(f"\n{SUBHEADER_COLOR}ä¸€ã€å†…å®¹çœŸå®æ€§ä¸å‡†ç¡®æ€§åˆ†æ{RESET_COLOR}")
        print(f"{DETAIL_COLOR}{'â”' * 70}{RESET_COLOR}")
        
        # AIç”Ÿæˆå†…å®¹æ£€æµ‹éƒ¨åˆ†
        print(f"\n{SECTION_COLOR}1. AIç”Ÿæˆå†…å®¹æ£€æµ‹:{RESET_COLOR}")
        if ai_content_data and isinstance(ai_content_data, dict):
            try:
                # åˆå§‹åŒ–ai_scoreå˜é‡
                ai_score = None
                
                # è®¡ç®—å¹³å‡åˆ†æ•°
                detailed_scores = {
                    "è¡¨è¾¾æ¨¡å¼": ai_content_data.get("expression_pattern", ai_content_data.get("è¡¨è¾¾æ¨¡å¼", 0)),
                    "è¯æ±‡å¤šæ ·æ€§": ai_content_data.get("vocabulary_diversity", ai_content_data.get("è¯æ±‡å¤šæ ·æ€§", 0)),
                    "å¥å­å˜åŒ–": ai_content_data.get("sentence_variation", ai_content_data.get("å¥å­å˜åŒ–", 0)),
                    "ä¸Šä¸‹æ–‡è¿è´¯æ€§": ai_content_data.get("context_coherence", ai_content_data.get("ä¸Šä¸‹æ–‡è¿è´¯æ€§", 0)),
                    "äººç±»ç‰¹å¾": ai_content_data.get("human_traits", ai_content_data.get("äººç±»ç‰¹å¾", 0))
                }
                
                valid_scores = [score for score in detailed_scores.values() if isinstance(score, (int, float)) and 0 <= float(score) <= 1]
                if valid_scores:
                    avg_score = sum(valid_scores) / len(valid_scores)
                    ai_score = avg_score  # è®¾ç½®ai_score
                    score_color = SUCCESS_COLOR if avg_score >= 0.8 else (WARNING_COLOR if avg_score >= 0.6 else ERROR_COLOR)
                    
                    print(f"{score_color}  â€¢ äººç±»å†™ä½œç‰¹å¾è¯„åˆ†: {avg_score:.2f} {get_progress_bar(avg_score)}{RESET_COLOR}")
                    print(f"{DETAIL_COLOR}    - {get_ai_content_description(avg_score)}{RESET_COLOR}")
                    
                    # æ˜¾ç¤ºAIç”Ÿæˆæ¦‚ç‡
                    ai_probability = max(0, min(1, 1 - avg_score))
                    ai_prob_color = SUCCESS_COLOR if ai_probability <= 0.3 else (WARNING_COLOR if ai_probability <= 0.5 else ERROR_COLOR)
                    print(f"{ai_prob_color}  â€¢ AIç”Ÿæˆæ¦‚ç‡: {ai_probability:.2f} {get_progress_bar(ai_probability)}{RESET_COLOR}")
                    
                    # æ˜¾ç¤ºè¯¦ç»†ç‰¹å¾è¯„åˆ†
                    print(f"\n{SECTION_COLOR}  AIç‰¹å¾è¯¦ç»†åˆ†æ:{RESET_COLOR}")
                    for key, score in detailed_scores.items():
                        if isinstance(score, (int, float)) and 0 <= float(score) <= 1:
                            score_float = float(score)
                            score_color = SUCCESS_COLOR if score_float >= 0.7 else (WARNING_COLOR if score_float >= 0.5 else ERROR_COLOR)
                            print(f"{score_color}    â€¢ {key}: {score_float:.2f} {get_progress_bar(score_float)}{RESET_COLOR}")
            except Exception as e:
                logger.error(f"æå–AIç”Ÿæˆå†…å®¹è¯„åˆ†æ—¶å‡ºé”™: {str(e)}")
                ai_score = None
                
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæ£€æŸ¥åµŒå¥—å­—å…¸
            if ai_score is None:
                try:
                    for key, value in ai_content_data.items():
                        if isinstance(value, dict) and "score" in value:
                            ai_score = value["score"]
                            logger.info(f"ä»åµŒå¥—å­—å…¸ä¸­æ‰¾åˆ°AIç”Ÿæˆå†…å®¹è¯„åˆ†ï¼Œé”®è·¯å¾„: {key}.score")
                            break
                except Exception as e:
                    logger.error(f"ä»åµŒå¥—å­—å…¸æå–AIè¯„åˆ†æ—¶å‡ºé”™: {str(e)}")
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ‰¾ç»“è®ºå­—æ®µ
            if ai_score is None:
                try:
                    conclusion = None
                    for key in ["conclusion", "summary", "ç»“è®º", "åˆ†æç»“æœ"]:
                        if key in ai_content_data:
                            conclusion = ai_content_data[key]
                            logger.info(f"æ‰¾åˆ°AIåˆ†æç»“è®ºï¼Œé”®å: {key}")
                            break
                    
                    if conclusion:
                        # ä»ç»“è®ºæ–‡æœ¬ä¸­æå–å¯èƒ½çš„è¯„åˆ†
                        if isinstance(conclusion, str):
                            import re
                            # å°è¯•æ‰¾å‡ºç±»ä¼¼ "0.7" æˆ– "70%" çš„è¯„åˆ†
                            score_match = re.search(r'(\d+(\.\d+)?)%?', conclusion)
                            if score_match:
                                try:
                                    potential_score = float(score_match.group(1))
                                    # å¦‚æœæ˜¯ç™¾åˆ†æ¯”æ ¼å¼ï¼Œè½¬æ¢ä¸º0-1èŒƒå›´
                                    if "%" in conclusion and potential_score > 1:
                                        potential_score /= 100
                                    # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
                                    ai_score = max(0, min(1, potential_score))
                                    logger.info(f"ä»ç»“è®ºä¸­æå–AIç”Ÿæˆå†…å®¹è¯„åˆ†: {ai_score}")
                                except ValueError:
                                    logger.warning(f"æ— æ³•ä»ç»“è®ºä¸­æå–è¯„åˆ†: {conclusion}")
                except Exception as e:
                    logger.error(f"å¤„ç†AIåˆ†æç»“è®ºæ—¶å‡ºé”™: {str(e)}")
                    
            if ai_score is not None:
                try:
                    ai_score_float = validate_score(ai_score, "AIç”Ÿæˆå†…å®¹è¯„åˆ†")
                    # æ³¨æ„ï¼šè¿™é‡Œè¯„åˆ†è¶Šé«˜ï¼Œè¡¨ç¤ºè¶Šåƒäººç±»å†™ä½œï¼ŒAIç”Ÿæˆå¯èƒ½æ€§è¶Šä½
                    score_color = SUCCESS_COLOR if ai_score_float >= 0.7 else (WARNING_COLOR if ai_score_float >= 0.5 else ERROR_COLOR)
                    print(f"{score_color}  â€¢ äººç±»å†™ä½œç‰¹å¾è¯„åˆ†: {ai_score_float:.2f} {get_progress_bar(ai_score_float)}{RESET_COLOR}")
                    print(f"{DETAIL_COLOR}  â€¢ {get_ai_content_description(ai_score_float)}{RESET_COLOR}")
                    
                    # æ·»åŠ AIç”Ÿæˆæ¦‚ç‡
                    ai_probability = max(0, min(1, 1 - ai_score_float))
                    ai_prob_color = SUCCESS_COLOR if ai_probability <= 0.3 else (WARNING_COLOR if ai_probability <= 0.5 else ERROR_COLOR)
                    print(f"{ai_prob_color}  â€¢ AIç”Ÿæˆæ¦‚ç‡: {ai_probability:.1%}{RESET_COLOR}")
                except ValueError:
                    logger.warning(f"AIç”Ÿæˆå†…å®¹è¯„åˆ†æ— æ•ˆ: {ai_score}")
                    print(f"{ERROR_COLOR}  â€¢ AIç”Ÿæˆå†…å®¹è¯„åˆ†æ— æ•ˆ{RESET_COLOR}")
            else:
                # å°è¯•ä»å…¶ä»–å­—æ®µæ¨æ–­AIç”Ÿæˆå¯èƒ½æ€§
                conclusion = None
                try:
                    for key in ["conclusion", "summary", "ç»“è®º", "åˆ†æç»“æœ"]:
                        if key in ai_content_data:
                            conclusion = ai_content_data[key]
                            logger.info(f"æ‰¾åˆ°AIåˆ†æç»“è®ºï¼Œé”®å: {key}")
                            break
                    
                    if conclusion:
                        print(f"{DETAIL_COLOR}  â€¢ AIåˆ†æç»“è®º: {conclusion}{RESET_COLOR}")
                        
                        # å°è¯•ä»ç»“è®ºä¸­æ¨æ–­AIç”Ÿæˆå¯èƒ½æ€§
                        if isinstance(conclusion, str):
                            if any(term in conclusion.lower() for term in ["äººå·¥æ™ºèƒ½ç”Ÿæˆ", "aiç”Ÿæˆ", "æœºå™¨ç”Ÿæˆ", "å¾ˆå¯èƒ½æ˜¯ai", "ç”Ÿæˆå¼ai"]):
                                print(f"{WARNING_COLOR}  â€¢ æ¨æ–­ç»“æœ: æ–‡æœ¬å¾ˆå¯èƒ½ç”±AIç”Ÿæˆ{RESET_COLOR}")
                            elif any(term in conclusion.lower() for term in ["éƒ¨åˆ†ç‰¹å¾", "æ··åˆç‰¹å¾", "aiè¾…åŠ©"]):
                                print(f"{WARNING_COLOR}  â€¢ æ¨æ–­ç»“æœ: æ–‡æœ¬å¯èƒ½æ˜¯AIè¾…åŠ©åˆ›ä½œ{RESET_COLOR}")
                            elif any(term in conclusion.lower() for term in ["äººç±»ç‰¹å¾", "äººå·¥æ’°å†™", "çœŸå®ä½œè€…"]):
                                print(f"{SUCCESS_COLOR}  â€¢ æ¨æ–­ç»“æœ: æ–‡æœ¬å…·æœ‰è¾ƒå¼ºçš„äººç±»å†™ä½œç‰¹å¾{RESET_COLOR}")
                    else:
                        print(f"{WARNING_COLOR}  â€¢ æœªæ‰¾åˆ°æ˜ç¡®çš„AIç”Ÿæˆå†…å®¹è¯„åˆ†æˆ–ç»“è®º{RESET_COLOR}")
                        print(f"{DETAIL_COLOR}  â€¢ ä»¥ä¸‹ä¸ºåŸå§‹AIæ£€æµ‹æ•°æ®çš„å…³é”®å­—æ®µ:{RESET_COLOR}")
                        # æ˜¾ç¤ºå…³é”®å­—æ®µï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£æ•°æ®
                        key_fields = [k for k in ai_content_data.keys() if k not in ["raw_data", "detail_data"]][:5]
                        for k in key_fields:
                            print(f"{DETAIL_COLOR}    - {k}: {str(ai_content_data[k])[:50]}...{RESET_COLOR}")
                except Exception as e:
                    logger.error(f"å¤„ç†AIåˆ†æç»“è®ºæ¨æ–­æ—¶å‡ºé”™: {str(e)}")
                    print(f"{ERROR_COLOR}  â€¢ å¤„ç†AIåˆ†æç»“è®ºæ—¶å‡ºé”™: {str(e)}{RESET_COLOR}")
                
                # æ·»åŠ AIè¯¦ç»†åˆ†æéƒ¨åˆ†ï¼Œæ”¾åœ¨if/elseçš„å¤–éƒ¨ï¼Œå› ä¸ºæ— è®ºæ˜¯å¦æ‰¾åˆ°ai_scoreéƒ½åº”å°è¯•åˆ†æè¯¦ç»†ç‰¹å¾
                try:
                    # å°è¯•æå–è¯¦ç»†çš„ç‰¹å¾åˆ†æ
                    detailed_scores = None
                    # å°è¯•ä»ä¸åŒçš„å¯èƒ½å­—æ®µåè·å–è¯¦ç»†è¯„åˆ†
                    for key in ["detailed_analysis", "deepseek_scores", "scores", "è¯¦ç»†è¯„åˆ†", "features", "åˆ†é¡¹è¯„åˆ†"]:
                        if key in ai_content_data and ai_content_data[key]:
                            detailed_scores = ai_content_data[key]
                            logger.info(f"æ‰¾åˆ°AIç”Ÿæˆå†…å®¹è¯¦ç»†è¯„åˆ†ï¼Œé”®å: {key}")
                            break
                    
                    # å¦‚æœç›´æ¥é”®æ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨åµŒå¥—å­—å…¸ä¸­æŸ¥æ‰¾
                    if not detailed_scores:
                        for key, value in ai_content_data.items():
                            if isinstance(value, dict) and any(sk in value for sk in ["score", "expression_pattern", "vocabulary_diversity", "äººç±»ç‰¹å¾"]):
                                detailed_scores = value
                                logger.info(f"ä»åµŒå¥—å­—å…¸ä¸­æ‰¾åˆ°AIç”Ÿæˆå†…å®¹è¯¦ç»†è¯„åˆ†: {key}")
                                break
                    
                    # å¦‚æœæ‰¾åˆ°äº†è¯¦ç»†ç‰¹å¾è¯„åˆ†ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°æ€»ä½“è¯„åˆ†ï¼Œåˆ™è®¡ç®—å¹³å‡å€¼ä½œä¸ºæ€»ä½“è¯„åˆ†
                    if detailed_scores and isinstance(detailed_scores, dict) and ai_score is None:
                        feature_scores = []
                        for key, value in detailed_scores.items():
                            try:
                                if isinstance(value, (int, float)) and 0 <= float(value) <= 1:
                                    feature_scores.append(float(value))
                            except:
                                pass
                        
                        if feature_scores:
                            # è®¡ç®—å¹³å‡å€¼ä½œä¸ºæ€»ä½“è¯„åˆ†
                            avg_score = sum(feature_scores) / len(feature_scores)
                            ai_score = avg_score
                            logger.info(f"ä»è¯¦ç»†ç‰¹å¾è¯„åˆ†è®¡ç®—å‡ºæ€»ä½“è¯„åˆ†: {ai_score:.2f}")
                            
                            # æ˜¾ç¤ºè®¡ç®—å¾—å‡ºçš„æ€»ä½“è¯„åˆ†
                            ai_score_float = avg_score
                            score_color = SUCCESS_COLOR if ai_score_float >= 0.7 else (WARNING_COLOR if ai_score_float >= 0.5 else ERROR_COLOR)
                            print(f"{score_color}  â€¢ äººç±»å†™ä½œç‰¹å¾è¯„åˆ†: {ai_score_float:.2f} {get_progress_bar(ai_score_float)}{RESET_COLOR}")
                            print(f"{DETAIL_COLOR}  â€¢ {get_ai_content_description(ai_score_float)}{RESET_COLOR}")
                            
                            # æ·»åŠ AIç”Ÿæˆæ¦‚ç‡
                            ai_probability = max(0, min(1, 1 - ai_score_float))
                            ai_prob_color = SUCCESS_COLOR if ai_probability <= 0.3 else (WARNING_COLOR if ai_probability <= 0.5 else ERROR_COLOR)
                            print(f"{ai_prob_color}  â€¢ AIç”Ÿæˆæ¦‚ç‡: {ai_probability:.1%}{RESET_COLOR}")
                    
                    # æ˜¾ç¤ºè¯¦ç»†ç‰¹å¾åˆ†æ
                    if detailed_scores and isinstance(detailed_scores, dict):
                        print(f"\n{SECTION_COLOR}  AIç‰¹å¾è¯¦ç»†åˆ†æ:{RESET_COLOR}")
                        
                        # å®šä¹‰å¸¸è§è¯„åˆ†é¡¹çš„æè¿°
                        score_descriptions = {
                            "expression_pattern": "è¡¨è¾¾æ¨¡å¼ (å¥å¼ç»“æ„çš„äººç±»ç‰¹å¾)",
                            "vocabulary_diversity": "è¯æ±‡å¤šæ ·æ€§ (ç”¨è¯ä¸°å¯Œåº¦çš„äººç±»ç‰¹å¾)",
                            "sentence_variation": "å¥å­å˜åŒ– (å¥å¼å˜åŒ–å¤šæ ·æ€§çš„äººç±»ç‰¹å¾)",
                            "context_coherence": "ä¸Šä¸‹æ–‡è¿è´¯æ€§ (é€»è¾‘æµç•…åº¦çš„äººç±»ç‰¹å¾)",
                            "human_traits": "äººç±»ç‰¹å¾ (æ–‡æœ¬ä¸­çš„äººç±»æ€ç»´ç‰¹å¾)",
                            "è¡¨è¾¾æ¨¡å¼": "è¡¨è¾¾æ¨¡å¼ (å¥å¼ç»“æ„çš„äººç±»ç‰¹å¾)",
                            "è¯æ±‡å¤šæ ·æ€§": "è¯æ±‡å¤šæ ·æ€§ (ç”¨è¯ä¸°å¯Œåº¦çš„äººç±»ç‰¹å¾)",
                            "å¥å­å˜åŒ–": "å¥å­å˜åŒ– (å¥å¼å˜åŒ–å¤šæ ·æ€§çš„äººç±»ç‰¹å¾)",
                            "ä¸Šä¸‹æ–‡è¿è´¯æ€§": "ä¸Šä¸‹æ–‡è¿è´¯æ€§ (é€»è¾‘æµç•…åº¦çš„äººç±»ç‰¹å¾)",
                            "äººç±»ç‰¹å¾": "äººç±»ç‰¹å¾ (æ–‡æœ¬ä¸­çš„äººç±»æ€ç»´ç‰¹å¾)"
                        }
                        
                        # è®¡ç®—æœ‰æ•ˆè¯„åˆ†é¡¹
                        valid_scores = 0
                        for key, value in detailed_scores.items():
                            try:
                                if isinstance(value, (int, float)) and 0 <= float(value) <= 1:
                                    valid_scores += 1
                            except:
                                pass
                        
                        if valid_scores > 0:
                            for key, value in detailed_scores.items():
                                try:
                                    if isinstance(value, (int, float)):
                                        score = validate_score(value, f"AIç”Ÿæˆå†…å®¹.{key}")
                                        score_color = SUCCESS_COLOR if score >= 0.7 else (WARNING_COLOR if score >= 0.5 else ERROR_COLOR)
                                        key_description = score_descriptions.get(key, key)
                                        print(f"{score_color}    â€¢ {key_description}: {score:.2f} {get_progress_bar(score)}{RESET_COLOR}")
                                except ValueError:
                                    logger.warning(f"AIç”Ÿæˆå†…å®¹è¯¦ç»†è¯„åˆ†'{key}'æ— æ•ˆ: {value}")
                        else:
                            print(f"{WARNING_COLOR}    â€¢ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¯¦ç»†è¯„åˆ†é¡¹{RESET_COLOR}")
                    
                    # å°è¯•æå–æ–‡æœ¬åˆ†æç»“è®º
                    text_analysis = None
                    for key in ["analysis", "deepseek_analysis", "text_analysis", "åˆ†æç»“è®º", "analysis_details", "æ£€æµ‹ç»“è®º", "ç»“è®ºè¯´æ˜"]:
                        if key in ai_content_data and ai_content_data[key]:
                            text_analysis = ai_content_data[key]
                            logger.info(f"æ‰¾åˆ°AIç”Ÿæˆå†…å®¹æ–‡æœ¬åˆ†æï¼Œé”®å: {key}")
                            break
                    
                    if text_analysis:
                        print(f"\n{SECTION_COLOR}  åˆ†æç»“è®º:{RESET_COLOR}")
                        if isinstance(text_analysis, str):
                            print(f"{DETAIL_COLOR}    â€¢ {text_analysis}{RESET_COLOR}")
                        elif isinstance(text_analysis, list):
                            for point in text_analysis:
                                print(f"{DETAIL_COLOR}    â€¢ {point}{RESET_COLOR}")
                except Exception as e:
                    logger.error(f"å¤„ç†AIç”Ÿæˆå†…å®¹æ£€æµ‹æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                    print(f"{ERROR_COLOR}  â€¢ AIç”Ÿæˆå†…å®¹æ£€æµ‹æ•°æ®å¤„ç†é”™è¯¯: {str(e)}{RESET_COLOR}")
                    # å°è¯•ç›´æ¥æ˜¾ç¤ºåŸå§‹æ•°æ®
                    try:
                        print(f"{DETAIL_COLOR}  â€¢ åŸå§‹AIç”Ÿæˆå†…å®¹æ•°æ®: {str(ai_content_data)[:200]}...{RESET_COLOR}")
                    except:
                        pass
        
        # 2. å¼•ç”¨åˆ†æå¤„ç†
        logger.debug("å¼€å§‹å¤„ç†å¼•ç”¨åˆ†ææ•°æ®")
        print(f"\n{SECTION_COLOR}1. å¼•ç”¨è´¨é‡åˆ†æ:{RESET_COLOR}")
        
        # å°è¯•ä»ä¸åŒä½ç½®è·å–å¼•ç”¨æ•°æ®
        citation_data = None
        
        # å°è¯•è·å–æ–°æ ¼å¼çš„å¼•ç”¨éªŒè¯ç»“æœ
        citation_validation_data = result.get("å¼•ç”¨éªŒè¯ç»“æœ", {})
        
        # å…ˆæ£€æŸ¥å¼•ç”¨éªŒè¯ç»“æœ
        if citation_validation_data:
            logger.info("ä½¿ç”¨å¼•ç”¨éªŒè¯ç»“æœä½œä¸ºå¼•ç”¨åˆ†ææ•°æ®")
            citation_data = citation_validation_data
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–é”®
        if not citation_data:
            for key in ["citation_details", "å¼•ç”¨éªŒè¯", "éªŒè¯ç»“æœ"]:
                if key in result and result[key]:
                    citation_data = result[key]
                    logger.info(f"æ‰¾åˆ°å¼•ç”¨åˆ†ææ•°æ®ï¼Œé”®å: {key}")
                    break
        
        # å¤„ç†å¼•ç”¨æ•°æ®
        if citation_data and isinstance(citation_data, dict):
            try:
                # å¤„ç†æ–°æ ¼å¼çš„å¼•ç”¨éªŒè¯ç»“æœ
                if "å¼•ç”¨éªŒè¯è¯„åˆ†" in citation_data and "é™ˆè¿°è¯¦æƒ…" in citation_data:
                    # æ˜¾ç¤ºæ€»ä½“è¯„åˆ†
                    score = citation_data.get("å¼•ç”¨éªŒè¯è¯„åˆ†", 0.5)
                    score_color = SUCCESS_COLOR if score >= 0.8 else (WARNING_COLOR if score >= 0.6 else ERROR_COLOR)
                    print(f"{score_color}  â€¢ å¼•ç”¨éªŒè¯è¯„åˆ†: {score:.2f} {get_progress_bar(score)}{RESET_COLOR}")
                    
                    # æ˜¾ç¤ºæ€»ç»“
                    if "æ€»ç»“" in citation_data:
                        print(f"{DETAIL_COLOR}  â€¢ æ€»ç»“: {citation_data['æ€»ç»“']}{RESET_COLOR}")
                    
                    # æ˜¾ç¤ºé™ˆè¿°è¯¦æƒ…
                    if isinstance(citation_data["é™ˆè¿°è¯¦æƒ…"], list):
                        print(f"\n{SECTION_COLOR}é™ˆè¿°éªŒè¯è¯¦æƒ…:{RESET_COLOR}")
                        for i, statement in enumerate(citation_data["é™ˆè¿°è¯¦æƒ…"], 1):
                            if isinstance(statement, dict):
                                content = statement.get("å†…å®¹", "æœªçŸ¥å†…å®¹")[:100]
                                status = statement.get("éªŒè¯çŠ¶æ€", "æœªéªŒè¯")
                                importance = statement.get("é‡è¦æ€§", "ä¸­")
                                support = statement.get("æ”¯æŒåº¦", 0)
                                
                                # è®¾ç½®é¢œè‰²
                                status_color = SUCCESS_COLOR if status.startswith("å·²éªŒè¯") else (WARNING_COLOR if status.startswith("éƒ¨åˆ†") else ERROR_COLOR)
                                imp_color = ERROR_COLOR if importance == "é«˜" else (WARNING_COLOR if importance == "ä¸­" else DETAIL_COLOR)
                                
                                # æ˜¾ç¤ºé™ˆè¿°å†…å®¹å’ŒéªŒè¯çŠ¶æ€
                                print(f"{DETAIL_COLOR}  â€¢ é™ˆè¿° {i}:{RESET_COLOR}")
                                print(f"{DETAIL_COLOR}    - å†…å®¹: {content}{RESET_COLOR}")
                                print(f"{imp_color}    - é‡è¦æ€§: {importance}{RESET_COLOR}")
                                print(f"{status_color}    - éªŒè¯çŠ¶æ€: {status}{RESET_COLOR}")
                                
                                if support > 0:
                                    support_color = SUCCESS_COLOR if support >= 0.7 else (WARNING_COLOR if support >= 0.4 else ERROR_COLOR)
                                    print(f"{support_color}    - æ”¯æŒåº¦: {support:.2f} {get_progress_bar(support)}{RESET_COLOR}")
                                
                                # æ˜¾ç¤ºæœç´¢ç»“æœ
                                if "æœç´¢ç»“æœ" in statement and isinstance(statement["æœç´¢ç»“æœ"], list):
                                    results = statement["æœç´¢ç»“æœ"]
                                    if results:
                                        print(f"{DETAIL_COLOR}    - ç›¸å…³ç»“æœ:{RESET_COLOR}")
                                        for j, result_item in enumerate(results[:2], 1):  # æ˜¾ç¤ºå‰ä¸¤ä¸ª
                                            if isinstance(result_item, dict):
                                                title = result_item.get("title", "æœªçŸ¥æ ‡é¢˜")
                                                url = result_item.get("url", "")
                                                print(f"{INFO_COLOR}      {j}. {title}{RESET_COLOR}")
                                                if url:
                                                    print(f"{INFO_COLOR}         {url}{RESET_COLOR}")
                                        
                                        if len(results) > 2:
                                            print(f"{DETAIL_COLOR}      ... ç­‰å…± {len(results)} ä¸ªç›¸å…³ç»“æœ{RESET_COLOR}")
                        
                        # æ˜¾ç¤ºéªŒè¯ç»Ÿè®¡
                        verified_count = sum(1 for s in citation_data["é™ˆè¿°è¯¦æƒ…"] if isinstance(s, dict) and s.get("éªŒè¯çŠ¶æ€", "").startswith("å·²éªŒè¯"))
                        total_count = len(citation_data["é™ˆè¿°è¯¦æƒ…"])
                        verification_ratio = verified_count / total_count if total_count > 0 else 0
                        
                        ratio_color = SUCCESS_COLOR if verification_ratio >= 0.7 else (WARNING_COLOR if verification_ratio >= 0.5 else ERROR_COLOR)
                        print(f"\n{ratio_color}  â€¢ éªŒè¯ç»Ÿè®¡: {verified_count}/{total_count} é™ˆè¿°å¾—åˆ°éªŒè¯ ({verification_ratio:.1%}){RESET_COLOR}")
                
                # å¤„ç†æ—§æ ¼å¼
                elif "citation_details" in citation_data:
                    # æ—§æ ¼å¼å¤„ç†
                    print(f"\n{SECTION_COLOR}å¼•ç”¨è¯¦æƒ…:{RESET_COLOR}")
                    for i, cite in enumerate(citation_data["citation_details"], 1):
                        logger.debug(f"å¤„ç†ç¬¬{i}ä¸ªå¼•ç”¨: {cite}")
                        verified = cite.get('verified', False)
                        status_color = SUCCESS_COLOR if verified else WARNING_COLOR
                        print(f"{DETAIL_COLOR}  â€¢ å¼•ç”¨{i}:{RESET_COLOR}")
                        print(f"{DETAIL_COLOR}    - å†…å®¹: {cite.get('quote', 'æœªçŸ¥')[:100]}...{RESET_COLOR}")
                        print(f"{DETAIL_COLOR}    - æ¥æº: {cite.get('source', 'æœªçŸ¥')}{RESET_COLOR}")
                        print(f"{status_color}    - éªŒè¯çŠ¶æ€: {'âœ“ å·²éªŒè¯' if verified else 'âœ— æœªéªŒè¯'}{RESET_COLOR}")
                        if "verification_method" in cite:
                            print(f"{DETAIL_COLOR}    - éªŒè¯æ–¹æ³•: {cite['verification_method']}{RESET_COLOR}")
                        if "confidence" in cite:
                            print(f"{DETAIL_COLOR}    - ç½®ä¿¡åº¦: {cite['confidence']:.2%}{RESET_COLOR}")
            except Exception as e:
                logger.error(f"å¤„ç†å¼•ç”¨åˆ†ææ—¶å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())
                print(f"{ERROR_COLOR}  â€¢ å¼•ç”¨æ•°æ®å¤„ç†é”™è¯¯: {str(e)}{RESET_COLOR}")
        else:
            print(f"{WARNING_COLOR}  â€¢ æœªæ‰¾åˆ°å¼•ç”¨åˆ†ææ•°æ®{RESET_COLOR}")
            print(f"{DETAIL_COLOR}  â€¢ å»ºè®®ï¼šè¿›è¡Œå¼•ç”¨åˆ†æä»¥æé«˜å†…å®¹å¯ä¿¡åº¦è¯„ä¼°{RESET_COLOR}")
        
        # 6. å¤„ç†æ¥æºè¯„åˆ†
        print(f"\n{SECTION_COLOR}2. æ¥æºè¯„åˆ†ä¸åˆ†æ:{RESET_COLOR}")
        
        # å°è¯•ä»ä¸åŒä½ç½®æå–æ¥æºæ•°æ®
        source_quality_data = None
        for key in ["source_quality", "æ¥æºè´¨é‡", "domain_credibility", "åŸŸåå¯ä¿¡åº¦"]:
            if key in result and result[key]:
                source_quality_data = result[key]
                logger.info(f"æ‰¾åˆ°æ¥æºè´¨é‡æ•°æ®ï¼Œé”®å: {key}")
                break
        
        # æ˜¾ç¤ºæ—¶æ•ˆæ€§
        timeliness = cross_validation_data.get("timeliness", cross_validation_data.get("æ—¶æ•ˆæ€§", "æœªçŸ¥"))
        print(f"{DETAIL_COLOR}  â€¢ æ—¶æ•ˆæ€§è¯„ä¼°: {timeliness}{RESET_COLOR}")
        
        # æ˜¾ç¤ºå¯ä¿¡å†…å®¹æ€»ç»“ (æ–°å¢éƒ¨åˆ†)
        if "å¯ä¿¡å†…å®¹æ€»ç»“" in cross_validation_data and cross_validation_data["å¯ä¿¡å†…å®¹æ€»ç»“"]:
            print(f"\n{SECTION_COLOR}å¯ä¿¡å†…å®¹æ€»ç»“:{RESET_COLOR}")
            summary = cross_validation_data["å¯ä¿¡å†…å®¹æ€»ç»“"]
            # ä½¿ç”¨é†’ç›®é¢œè‰²æ˜¾ç¤ºæ€»ç»“
            print(f"{SUCCESS_COLOR}  {summary}{RESET_COLOR}")
        
        # æ˜¾ç¤ºé—®é¢˜ç‚¹ - ä¿®æ”¹æ­¤éƒ¨åˆ†
        # è®¡ç®—éªŒè¯ç‚¹ä¸­æ— ç»“æœçš„æ•°é‡å’Œæ¥æºæ•°é‡
        no_result_count = 0
        source_count = 0
        search_results_count = 0
        
        # é¦–å…ˆå°è¯•è®¡ç®—æ— ç»“æœçš„éªŒè¯ç‚¹
        if verification_points and isinstance(verification_points, list):
            no_result_count = sum(1 for p in verification_points if isinstance(p, dict) and p.get("æœç´¢ç»“æœæ•°é‡", 0) == 0)
            
            # åŒæ—¶å°è¯•ä»éªŒè¯ç‚¹ä¸­è·å–æœç´¢ç»“æœæ€»æ•°
            for p in verification_points:
                if isinstance(p, dict) and "æœç´¢ç»“æœ" in p and isinstance(p["æœç´¢ç»“æœ"], int):
                    search_results_count += p["æœç´¢ç»“æœ"]
                elif isinstance(p, dict) and "æœç´¢ç»“æœæ•°é‡" in p and isinstance(p["æœç´¢ç»“æœæ•°é‡"], int):
                    search_results_count += p["æœç´¢ç»“æœæ•°é‡"]
        
        # å°è¯•è·å–æ¥æºæ•°é‡
        # ç›´æ¥ä½¿ç”¨æ¥æºåˆ—è¡¨é•¿åº¦
        if sources and isinstance(sources, list):
            source_count = len(sources)
        # å°è¯•ä»äº¤å‰éªŒè¯æ•°æ®ä¸­è·å–æ¥æºæ•°é‡
        else:
            # å°è¯•å¤šç§å¯èƒ½çš„é”®å
            for key in ["source_count", "sources_count", "æœç´¢ç»“æœæ€»æ•°", "æ¥æºæ•°é‡", "ç›¸å…³æ¥æºæ•°"]:
                if key in cross_validation_data and isinstance(cross_validation_data[key], (int, float, str)):
                    try:
                        source_count = int(cross_validation_data[key])
                        break
                    except (ValueError, TypeError):
                        pass
        
        # å¦‚æœæœç´¢ç»“æœæ•°å¤§äº0ä½†æ¥æºè®¡æ•°ä¸º0ï¼Œä½¿ç”¨æœç´¢ç»“æœæ•°ä½œä¸ºæ¥æºè®¡æ•°çš„ä¼°è®¡
        if search_results_count > 0 and source_count == 0:
            logger.info(f"ä½¿ç”¨æœç´¢ç»“æœæ•°é‡({search_results_count})ä½œä¸ºæ¥æºæ•°é‡ä¼°è®¡")
            source_count = search_results_count
        
        # åªæœ‰åœ¨ç¡®å®æœ‰éªŒè¯ç‚¹ä½†æ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œæˆ–æ¥æºå¤ªå°‘æ—¶æ‰æ˜¾ç¤ºé—®é¢˜
        has_problems = False
        
        if verification_points:
            # åªæœ‰å½“æ¥æºç¡®å®å¤ªå°‘(å°äº2)ä¸”æœç´¢ç»“æœä¹Ÿä¸è¶³æ—¶æ‰æ˜¾ç¤ºæ¥æºä¸è¶³é—®é¢˜
            if source_count < 2 and search_results_count < 3:
                has_problems = True
                print(f"\n{SECTION_COLOR}äº¤å‰éªŒè¯é—®é¢˜:{RESET_COLOR}")
                print(f"{WARNING_COLOR}  â€¢ ç¼ºä¹è¶³å¤Ÿçš„äº¤å‰éªŒè¯æ¥æº (ä»…{source_count}ä¸ª){RESET_COLOR}")
                print(f"{DETAIL_COLOR}    - å»ºè®®ï¼šå»ºè®®å¯»æ‰¾æ›´å¤šç‹¬ç«‹æ¥æºéªŒè¯ä¿¡æ¯{RESET_COLOR}")
            
            # æ— è®ºæ¥æºæ•°é‡å¦‚ä½•ï¼Œå¦‚æœæœ‰éªŒè¯ç‚¹æ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œéƒ½æ˜¾ç¤ºè¿™ä¸ªé—®é¢˜
            if no_result_count > 0:
                if not has_problems:
                    has_problems = True
                    print(f"\n{SECTION_COLOR}äº¤å‰éªŒè¯é—®é¢˜:{RESET_COLOR}")
                    
                print(f"{WARNING_COLOR}  â€¢ {no_result_count}ä¸ªéªŒè¯ç‚¹æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯{RESET_COLOR}")
                print(f"{DETAIL_COLOR}    - è¿™äº›éªŒè¯ç‚¹å¯èƒ½éœ€è¦é¢å¤–éªŒè¯{RESET_COLOR}")
                print(f"{DETAIL_COLOR}    - å»ºè®®ï¼šé’ˆå¯¹è¿™äº›ç‰¹å®šä¿¡æ¯ç‚¹è¿›è¡Œé¢å¤–éªŒè¯{RESET_COLOR}")
        
        # æ£€æŸ¥æƒé‡ä¸­æ˜¯å¦æœ‰äº¤å‰éªŒè¯çš„è´¡çŒ®
        if not has_cv_data and "äº¤å‰éªŒè¯" in weights:
            print(f"{SECTION_COLOR}äº¤å‰éªŒè¯è¯„ä¼°:{RESET_COLOR}")
            cv_weight = weights["äº¤å‰éªŒè¯"]
            print(f"{DETAIL_COLOR}  â€¢ äº¤å‰éªŒè¯åœ¨æ€»è¯„åˆ†ä¸­çš„è´¡çŒ®æ¯”ä¾‹: {cv_weight:.2f}{RESET_COLOR}")
            print(f"{WARNING_COLOR}  â€¢ äº¤å‰éªŒè¯è¯¦ç»†æ•°æ®ä¸å¯ç”¨ï¼Œä½†å·²çº³å…¥æ€»è¯„åˆ†è®¡ç®—{RESET_COLOR}")
            print(f"{DETAIL_COLOR}  â€¢ é»˜è®¤äº¤å‰éªŒè¯è¯„åˆ†ç”¨äºè®¡ç®—: 0.5{RESET_COLOR}")
            has_cv_data = True
        
        # æ— äº¤å‰éªŒè¯æ•°æ®çš„æƒ…å†µ
        if not has_cv_data:
            # æ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦è®°å½•äº†äº¤å‰éªŒè¯ä¿¡æ¯
            if "validation" in str(result).lower() or "äº¤å‰éªŒè¯" in str(result):
                print(f"{WARNING_COLOR}  â€¢ å‘ç°äº¤å‰éªŒè¯ç›¸å…³ä¿¡æ¯ï¼Œä½†æ ¼å¼æ— æ³•è§£æ{RESET_COLOR}")
                print(f"{DETAIL_COLOR}  â€¢ å»ºè®®ï¼šæŸ¥çœ‹æ—¥å¿—è·å–æ›´å¤šäº¤å‰éªŒè¯è¯¦æƒ…{RESET_COLOR}")
            else:
                print(f"{WARNING_COLOR}  â€¢ æ²¡æœ‰äº¤å‰éªŒè¯æ•°æ®{RESET_COLOR}")
                print(f"{DETAIL_COLOR}  â€¢ å»ºè®®ï¼šè€ƒè™‘å¯ç”¨äº¤å‰éªŒè¯åŠŸèƒ½ä»¥æé«˜åˆ†æå¯é æ€§{RESET_COLOR}")
                # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„äº¤å‰éªŒè¯æ•°æ®ç»“æ„ä¾›åˆ†æé—®é¢˜ä½¿ç”¨
                cross_validation_data = {"source_count": 0, "unique_sources": 0}
        
        # åˆ†æé—®é¢˜
        problems = analyze_problems(result, total_score, main_scores, cross_validation_data)
        
        # æ‰“å°é—®é¢˜åˆ†æéƒ¨åˆ†
        print(f"\n{SUBHEADER_COLOR}äº”ã€é—®é¢˜ç‚¹åˆ†æ{RESET_COLOR}")
        print(f"{DETAIL_COLOR}{'â”' * 70}{RESET_COLOR}")
        
        if not problems:
            print(f"{SUCCESS_COLOR}  âœ“ æœªå‘ç°æ˜æ˜¾é—®é¢˜{RESET_COLOR}")
            print(f"{DETAIL_COLOR}  â€¢ å»ºè®®ï¼šä¿æŒæ‰¹åˆ¤æ€§æ€ç»´ï¼Œå…³æ³¨ä¿¡æ¯æ›´æ–°{RESET_COLOR}")
        else:
            # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼ˆä¸¥é‡ > ä¸­ç­‰ï¼‰
            problems.sort(key=lambda x: 0 if x["severity"] == "ä¸¥é‡" else 1)
            
            for i, problem in enumerate(problems, 1):
                color = problem["color"]
                print(f"\n{color}{i}. {problem['type']}é—®é¢˜:{RESET_COLOR}")
                print(f"{color}  âš ï¸ ä¸¥é‡æ€§ï¼š{problem['severity']}{RESET_COLOR}")
                print(f"{color}    - {problem['description']}{RESET_COLOR}")
                print(f"{color}    - å»ºè®®ï¼š{problem['suggestion']}{RESET_COLOR}")
        
        # ç³»ç»Ÿè­¦å‘Š
        warnings = result.get("è­¦å‘Š", [])
        if warnings and isinstance(warnings, list):
            print(f"\n{SUBHEADER_COLOR}å…­ã€ç³»ç»Ÿè­¦å‘Š{RESET_COLOR}")
            print(f"{DETAIL_COLOR}{'â”' * 70}{RESET_COLOR}")
            for warning in warnings:
                if isinstance(warning, str):
                    logger.warning(f"ç³»ç»Ÿè­¦å‘Š: {warning}")
                    print(f"{WARNING_COLOR}  âš ï¸ {warning}{RESET_COLOR}")
            
            # æ·»åŠ äº¤å‰éªŒè¯éªŒè¯ç‚¹ç»Ÿè®¡ä¿¡æ¯
            if "äº¤å‰éªŒè¯" in result and isinstance(result["äº¤å‰éªŒè¯"], dict):
                if "éªŒè¯ç‚¹ç»Ÿè®¡" in result["äº¤å‰éªŒè¯"]:
                    stats = result["äº¤å‰éªŒè¯"]["éªŒè¯ç‚¹ç»Ÿè®¡"]
                    total_points = stats.get("æ€»æ•°", len(result["äº¤å‰éªŒè¯"].get("éªŒè¯ç‚¹", [])))
                    success_count = stats.get("éªŒè¯æˆåŠŸ", 0)
                    fail_count = stats.get("éªŒè¯å¤±è´¥", 0)
                    no_result_count = stats.get("æ— ç»“æœ", 0)
                    
                    if total_points > 0:
                        print(f"{INFO_COLOR}  â„¹ï¸ äº¤å‰éªŒè¯: å…±æœ‰{total_points}ä¸ªéªŒè¯ç‚¹ï¼Œå…¶ä¸­{success_count}ä¸ªé€šè¿‡éªŒè¯ï¼Œ{fail_count}ä¸ªéªŒè¯å¤±è´¥ï¼Œ{no_result_count}ä¸ªæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯{RESET_COLOR}")
                elif "éªŒè¯ç‚¹" in result["äº¤å‰éªŒè¯"]:
                    # å¦‚æœæ²¡æœ‰ç»Ÿè®¡æ•°æ®ä½†æœ‰éªŒè¯ç‚¹ï¼Œæˆ‘ä»¬è‡ªå·±è®¡ç®—
                    verification_points = result["äº¤å‰éªŒè¯"]["éªŒè¯ç‚¹"]
                    total_points = len(verification_points)
                    success_count = sum(1 for p in verification_points if isinstance(p, dict) and p.get("éªŒè¯è¯„åˆ†", 0) >= 0.7)
                    fail_count = sum(1 for p in verification_points if isinstance(p, dict) and p.get("éªŒè¯è¯„åˆ†", 0) < 0.4)
                    no_result_count = sum(1 for p in verification_points if isinstance(p, dict) and (p.get("æœç´¢ç»“æœæ•°é‡", 0) == 0 or (0.4 <= p.get("éªŒè¯è¯„åˆ†", 0) < 0.7)))
                    
                    if total_points > 0:
                        print(f"{INFO_COLOR}  â„¹ï¸ äº¤å‰éªŒè¯: å…±æœ‰{total_points}ä¸ªéªŒè¯ç‚¹ï¼Œå…¶ä¸­{success_count}ä¸ªé€šè¿‡éªŒè¯ï¼Œ{fail_count}ä¸ªéªŒè¯å¤±è´¥ï¼Œ{no_result_count}ä¸ªæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯{RESET_COLOR}")
        
        # åº•éƒ¨ä¿¡æ¯
        print(f"\n{HEADER_COLOR}{'=' * 70}{RESET_COLOR}")
        print(f"{HEADER_COLOR}{'åˆ†æå®Œæˆ - æ„Ÿè°¢ä½¿ç”¨æ–°é—»å¯ä¿¡åº¦åˆ†æå·¥å…·':^70}{RESET_COLOR}")
        print(f"{HEADER_COLOR}{'=' * 70}{RESET_COLOR}")
        
        logger.info("åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
    except Exception as e:
        logger.error(f"æ ¼å¼åŒ–ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
        print(f"{ERROR_COLOR}æ ¼å¼åŒ–ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}{RESET_COLOR}")
        print(f"{ERROR_COLOR}é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}{RESET_COLOR}")
        # å°è¯•æ‰“å°åŸå§‹æ•°æ®ä»¥ä¾¿è°ƒè¯•
        print(f"{ERROR_COLOR}åŸå§‹æ•°æ®:\n{result}{RESET_COLOR}")

def display_detailed_results(result: Dict[str, Any]) -> None:
    """æ˜¾ç¤ºè¯¦ç»†çš„åˆ†æç»“æœ"""
    
    # AIç”Ÿæˆå†…å®¹æ£€æµ‹
    print(f"\n{SECTION_COLOR}â–¶ AIç”Ÿæˆå†…å®¹æ£€æµ‹{RESET_COLOR}")
    if "ai_content" in result:
        ai_content = result["ai_content"]
        print(f"â€¢ ç»¼åˆè¯„åˆ†: {format_score(ai_content.get('score', 0))}")
        
        # DeepSeekå¤šç»´åº¦è¯„åˆ†
        if "deepseek_scores" in ai_content:
            print(f"â€¢ DeepSeekå¤šç»´åº¦è¯„åˆ† (AIç”Ÿæˆå†…å®¹):")
            scores = ai_content["deepseek_scores"]
            print(f"  - è¡¨è¾¾æ¨¡å¼: {format_score(scores.get('expression_pattern', 0))}")
            print(f"  - è¯æ±‡å¤šæ ·æ€§: {format_score(scores.get('vocabulary_diversity', 0))}")
            print(f"  - å¥å­å˜åŒ–: {format_score(scores.get('sentence_variation', 0))}")
            print(f"  - ä¸Šä¸‹æ–‡è¿è´¯æ€§: {format_score(scores.get('context_coherence', 0))}")
            print(f"  - äººç±»ç‰¹å¾: {format_score(scores.get('human_traits', 0))}")
        
        # DeepSeekåˆ†æ
        if "deepseek_analysis" in ai_content:
            print(f"\nâ€¢ DeepSeekåˆ†æ: {ai_content['deepseek_analysis']}")
    else:
        print(f"{ERROR_COLOR}  â€¢ é”™è¯¯ï¼šæ— æ³•è·å–AIç”Ÿæˆå†…å®¹æ£€æµ‹æ•°æ®{RESET_COLOR}")

    # è¯­è¨€ä¸­ç«‹æ€§
    print(f"\n{SECTION_COLOR}â–¶ è¯­è¨€ä¸­ç«‹æ€§{RESET_COLOR}")
    if "è¯­è¨€ä¸­ç«‹æ€§" in result:
        neutrality = result["è¯­è¨€ä¸­ç«‹æ€§"]
        print(f"â€¢ ç»¼åˆè¯„åˆ†: {format_score(neutrality.get('score', 0))}")
        
        # DeepSeekå¤šç»´åº¦è¯„åˆ†
        if "deepseek_scores" in neutrality:
            print(f"â€¢ DeepSeekå¤šç»´åº¦è¯„åˆ† (è¯­è¨€ä¸­ç«‹æ€§):")
            scores = neutrality["deepseek_scores"]
            print(f"  - æƒ…æ„Ÿè¯æ±‡: {format_score(scores.get('emotional_words', 0))}")
            print(f"  - æƒ…æ„Ÿå¹³è¡¡: {format_score(scores.get('sentiment_balance', 0))}")
            print(f"  - æç«¯è¡¨è¿°: {format_score(scores.get('extreme_expressions', 0))}")
            print(f"  - ç…½åŠ¨æ€§è¡¨è¾¾: {format_score(scores.get('inflammatory_expressions', 0))}")
            print(f"  - ä¸»è§‚è¯„ä»·: {format_score(scores.get('subjective_evaluation', 0))}")
        
        # DeepSeekåˆ†æ
        if "deepseek_analysis" in neutrality:
            print(f"\nâ€¢ DeepSeekåˆ†æ: {neutrality['deepseek_analysis']}")
    else:
        print(f"{ERROR_COLOR}  â€¢ é”™è¯¯ï¼šæ— æ³•è·å–è¯­è¨€ä¸­ç«‹æ€§åˆ†ææ•°æ®{RESET_COLOR}")

    # æ¥æºè´¨é‡
    print(f"\n{SECTION_COLOR}â–¶ æ¥æºè´¨é‡{RESET_COLOR}")
    if "source_quality" in result:
        source = result["source_quality"]
        if "domain_trust" in source:
            print(f"â€¢ {source['domain_trust']}")
        if "source_count" in source:
            print(f"â€¢ å¼•ç”¨äº†{get_source_level(source['source_count'])}çš„æ¥æº ({source['source_count']}ä¸ª)")
        if "authority_sources" in source:
            print(f"â€¢ {'å‘ç°' if source['authority_sources'] > 0 else 'æœªå‘ç°'}æƒå¨æ¥æºå¼•ç”¨")
        if "direct_quotes" in source:
            print(f"â€¢ åŒ…å«{'å¤šä¸ª' if source['direct_quotes'] > 3 else 'å°‘é‡'}ç›´æ¥å¼•ç”¨ ({source['direct_quotes']}ä¸ª)")
    else:
        print(f"{ERROR_COLOR}  â€¢ é”™è¯¯ï¼šæ— æ³•è·å–æ¥æºè´¨é‡åˆ†ææ•°æ®{RESET_COLOR}")

    # åŸŸåå¯ä¿¡åº¦
    print(f"\n{SECTION_COLOR}â–¶ åŸŸåå¯ä¿¡åº¦{RESET_COLOR}")
    if "domain_credibility" in result:
        domain = result["domain_credibility"]
        if "trust_level" in domain:
            print(f"â€¢ {domain['trust_level']}")
    else:
        print(f"{ERROR_COLOR}  â€¢ é”™è¯¯ï¼šæ— æ³•è·å–åŸŸåå¯ä¿¡åº¦æ•°æ®{RESET_COLOR}")

    # å¼•ç”¨æœ‰æ•ˆæ€§
    print(f"\n{SECTION_COLOR}â–¶ å¼•ç”¨æœ‰æ•ˆæ€§{RESET_COLOR}")
    if "citation_validity" in result:
        validity = result["citation_validity"]
        print(f"â€¢ å¼•ç”¨æ•°é‡: {get_citation_status(validity.get('citation_count', 0))}")
        print(f"â€¢ å¼•ç”¨å‡†ç¡®æ€§: {validity.get('accuracy_assessment', 'æ— æ³•è¯„ä¼°')}")
        print(f"â€¢ å¼•ç”¨å†…å®¹çš„çœŸå®æ€§è¯„ä¼°ï¼š{validity.get('authenticity_assessment', 'æ— æ³•è¯„ä¼°')}")
    else:
        print(f"{ERROR_COLOR}  â€¢ é”™è¯¯ï¼šæ— æ³•è·å–å¼•ç”¨æœ‰æ•ˆæ€§æ•°æ®{RESET_COLOR}")

    # å¼•ç”¨è´¨é‡
    print(f"\n{SECTION_COLOR}â–¶ å¼•ç”¨è´¨é‡{RESET_COLOR}")
    if "citation_quality" in result:
        quality = result["citation_quality"]
        print(f"â€¢ å¼•ç”¨æ•°é‡: {get_quantity_level(quality.get('total_citations', 0))} (ç›´æ¥å¼•è¯­: {quality.get('direct_quotes', 0)}, é—´æ¥å¼•ç”¨: {quality.get('indirect_quotes', 0)})")
        print(f"â€¢ å¼•ç”¨æ¥æºå¤šæ ·æ€§: {get_diversity_assessment(quality.get('unique_sources', 0))} (æ£€æµ‹åˆ°{quality.get('unique_sources', 0)}ä¸ªä¸åŒæ¥æº)")
        print(f"â€¢ å¼•ç”¨æ¥æºæƒå¨æ€§: {get_authority_level(quality.get('authority_sources', 0))} (æ£€æµ‹åˆ°{quality.get('authority_sources', 0)}ä¸ªæƒå¨æ¥æº)")
        print(f"â€¢ å¼•ç”¨è´¨é‡è¯„ä¼°ï¼š{quality.get('overall_assessment', 'æ— æ³•è¯„ä¼°')}")
    else:
        print(f"{ERROR_COLOR}  â€¢ é”™è¯¯ï¼šæ— æ³•è·å–å¼•ç”¨è´¨é‡æ•°æ®{RESET_COLOR}")

    # æœ¬åœ°æ–°é—»éªŒè¯
    print(f"\n{SECTION_COLOR}â–¶ æœ¬åœ°æ–°é—»éªŒè¯{RESET_COLOR}")
    if "local_verification" in result:
        local = result["local_verification"]
        print(f"â€¢ {local.get('assessment', 'æœªå‘ç°æ˜æ˜¾çš„æœ¬åœ°ç›¸å…³æ€§æŒ‡æ ‡')}")
    else:
        print(f"{ERROR_COLOR}  â€¢ é”™è¯¯ï¼šæ— æ³•è·å–æœ¬åœ°æ–°é—»éªŒè¯æ•°æ®{RESET_COLOR}")

    # é€»è¾‘åˆ†æ
    print(f"\n{SECTION_COLOR}â–¶ é€»è¾‘åˆ†æ{RESET_COLOR}")
    if "logic_analysis" in result:
        logic = result["logic_analysis"]
        for point in logic.get("points", []):
            print(f"â€¢ {point}")
    else:
        print(f"{ERROR_COLOR}  â€¢ é”™è¯¯ï¼šæ— æ³•è·å–é€»è¾‘åˆ†ææ•°æ®{RESET_COLOR}")

    # äº¤å‰éªŒè¯
    print(f"\n{SECTION_COLOR}â–¶ äº¤å‰éªŒè¯{RESET_COLOR}")
    if "cross_validation" in result:
        cross = result["cross_validation"]
        if "source_count" in cross:
            print(f"â€¢ æœç´¢åˆ°äº†{cross['unique_sources']}ä¸ªä¸åŒæ¥æºçš„{cross['source_count']}ç¯‡æŠ¥é“")
        if "timeliness" in cross:
            print(f"â€¢ {cross['timeliness']}")
        if "source_credibility" in cross:
            print(f"â€¢ {cross['source_credibility']}")
    else:
        print(f"{ERROR_COLOR}  â€¢ é”™è¯¯ï¼šæ— æ³•è·å–äº¤å‰éªŒè¯æ•°æ®{RESET_COLOR}")

def get_source_level(count: int) -> str:
    if count == 0:
        return "æ— "
    elif count < 3:
        return "æœ‰é™"
    elif count < 5:
        return "é€‚é‡"
    else:
        return "å……è¶³"

def get_citation_status(count: int) -> str:
    if count == 0:
        return "æ— æ˜ç¡®å¼•ç”¨"
    elif count < 3:
        return f"è¾ƒå°‘ ({count}ä¸ª)"
    elif count < 5:
        return f"é€‚é‡ ({count}ä¸ª)"
    else:
        return f"å……è¶³ ({count}ä¸ª)"

def get_quantity_level(count: int) -> str:
    if count == 0:
        return "æ— "
    elif count < 3:
        return "è¾ƒå°‘"
    elif count < 5:
        return "é€‚é‡"
    else:
        return "å……è¶³"

def get_diversity_assessment(count: int) -> str:
    if count == 0:
        return "æ— æ³•è¯„ä¼°"
    elif count < 2:
        return "å•ä¸€"
    elif count < 4:
        return "ä¸€èˆ¬"
    else:
        return "å¤šæ ·"

def get_authority_level(count: int) -> str:
    if count == 0:
        return "ä½"
    elif count < 2:
        return "ä¸€èˆ¬"
    elif count < 4:
        return "è¾ƒé«˜"
    else:
        return "é«˜" 